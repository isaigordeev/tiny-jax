{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "pretrained_weights = torchvision.models.AlexNet_Weights\n",
    "\n",
    "pretrained_alex_net = torchvision.models.alexnet(pretrained_weights).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "AlexNet (AlexNet)                        [1, 3, 224, 224]     [1, 1000]            --                   True\n",
       "├─Sequential (features)                  [1, 3, 224, 224]     [1, 256, 6, 6]       --                   True\n",
       "│    └─Conv2d (0)                        [1, 3, 224, 224]     [1, 64, 55, 55]      23,296               True\n",
       "│    └─ReLU (1)                          [1, 64, 55, 55]      [1, 64, 55, 55]      --                   --\n",
       "│    └─MaxPool2d (2)                     [1, 64, 55, 55]      [1, 64, 27, 27]      --                   --\n",
       "│    └─Conv2d (3)                        [1, 64, 27, 27]      [1, 192, 27, 27]     307,392              True\n",
       "│    └─ReLU (4)                          [1, 192, 27, 27]     [1, 192, 27, 27]     --                   --\n",
       "│    └─MaxPool2d (5)                     [1, 192, 27, 27]     [1, 192, 13, 13]     --                   --\n",
       "│    └─Conv2d (6)                        [1, 192, 13, 13]     [1, 384, 13, 13]     663,936              True\n",
       "│    └─ReLU (7)                          [1, 384, 13, 13]     [1, 384, 13, 13]     --                   --\n",
       "│    └─Conv2d (8)                        [1, 384, 13, 13]     [1, 256, 13, 13]     884,992              True\n",
       "│    └─ReLU (9)                          [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "│    └─Conv2d (10)                       [1, 256, 13, 13]     [1, 256, 13, 13]     590,080              True\n",
       "│    └─ReLU (11)                         [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "│    └─MaxPool2d (12)                    [1, 256, 13, 13]     [1, 256, 6, 6]       --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)            [1, 256, 6, 6]       [1, 256, 6, 6]       --                   --\n",
       "├─Sequential (classifier)                [1, 9216]            [1, 1000]            --                   True\n",
       "│    └─Dropout (0)                       [1, 9216]            [1, 9216]            --                   --\n",
       "│    └─Linear (1)                        [1, 9216]            [1, 4096]            37,752,832           True\n",
       "│    └─ReLU (2)                          [1, 4096]            [1, 4096]            --                   --\n",
       "│    └─Dropout (3)                       [1, 4096]            [1, 4096]            --                   --\n",
       "│    └─Linear (4)                        [1, 4096]            [1, 4096]            16,781,312           True\n",
       "│    └─ReLU (5)                          [1, 4096]            [1, 4096]            --                   --\n",
       "│    └─Linear (6)                        [1, 4096]            [1, 1000]            4,097,000            True\n",
       "========================================================================================================================\n",
       "Total params: 61,100,840\n",
       "Trainable params: 61,100,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 714.68\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 3.95\n",
       "Params size (MB): 244.40\n",
       "Estimated Total Size (MB): 248.96\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(pretrained_alex_net,\n",
    "        input_size=(1, 3, 224, 224),  # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'property' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m transforms \u001B[39m=\u001B[39m pretrained_weights\u001B[39m.\u001B[39mtransforms\n\u001B[1;32m      4\u001B[0m img \u001B[39m=\u001B[39m Image\u001B[39m.\u001B[39mopen(\u001B[39m'\u001B[39m\u001B[39mdog.jpg\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m _img \u001B[39m=\u001B[39m transforms(img)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'property' object is not callable"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "transforms = pretrained_weights.transforms\n",
    "img = Image.open('dog.jpg')\n",
    "_img = transforms(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (JpegImageFile, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!JpegImageFile!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!JpegImageFile!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m pretrained_alex_net(img)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/alexnet.py:48\u001B[0m, in \u001B[0;36mAlexNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, x: torch\u001B[39m.\u001B[39mTensor) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m torch\u001B[39m.\u001B[39mTensor:\n\u001B[0;32m---> 48\u001B[0m     x \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfeatures(x)\n\u001B[1;32m     49\u001B[0m     x \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mavgpool(x)\n\u001B[1;32m     50\u001B[0m     x \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mflatten(x, \u001B[39m1\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[39mfor\u001B[39;00m module \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[39minput\u001B[39m \u001B[39m=\u001B[39m module(\u001B[39minput\u001B[39;49m)\n\u001B[1;32m    218\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39minput\u001B[39m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[39m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[39m=\u001B[39m [], []\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39minput\u001B[39m: Tensor) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_conv_forward(\u001B[39minput\u001B[39;49m, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mweight, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mbias)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpadding_mode \u001B[39m!=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mzeros\u001B[39m\u001B[39m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39mconv2d(F\u001B[39m.\u001B[39mpad(\u001B[39minput\u001B[39m, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[39m0\u001B[39m), \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdilation, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39;49mconv2d(\u001B[39minput\u001B[39;49m, weight, bias, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mstride,\n\u001B[1;32m    460\u001B[0m                 \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mpadding, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdilation, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mgroups)\n",
      "\u001B[0;31mTypeError\u001B[0m: conv2d() received an invalid combination of arguments - got (JpegImageFile, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!JpegImageFile!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!JpegImageFile!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "pretrained_alex_net(_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
